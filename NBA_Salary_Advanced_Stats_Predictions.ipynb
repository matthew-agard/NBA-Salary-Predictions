{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WROzUUldhoit"
   },
   "source": [
    "Matthew Agard\n",
    "\n",
    "Machine Learning, CPTR475\n",
    "\n",
    "Final Project\n",
    "\n",
    "12/13/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wwo_90b9hoiw"
   },
   "source": [
    "# Understanding the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pfRyZKhhoix"
   },
   "source": [
    "https://www.kaggle.com/aishjun/nba-salaries-prediction-in-20172018-season\n",
    "\n",
    "Additional files required:\n",
    "- ``future_encoders.py``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4DiM8Wmhoi0"
   },
   "source": [
    "## Predicting NBA Player Salaries\n",
    "\n",
    "The \"2017-18_NBA_salary.csv\" dataset from Kaggle will be experimented upon in this notebook with the intent to unveil the correlation between an NBA player’s statistical output over the course of the 2017-18 NBA season and their salary for that same time period. \n",
    "\n",
    "It should be noted that this particular dataset isn’t comprised of traditional NBA stats such as points per game (PPG) or rebounds per game (RPG), but rather in advanced NBA stats such as win shares (WS) and player efficiency rating (PER). Using these advanced statistics is quickly becoming a growing trend in the NBA in deciding impactful team personnel outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mu1hE5gDhoi2"
   },
   "source": [
    "## Problem Structure\n",
    "\n",
    "This is a supervised regression problem because the dataset comes with the salary labels, which are continuous values that can be outputted as estimates by my models--which will be evaluated using the root mean squared error (RMSE) metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y82AdBcWz1bZ"
   },
   "source": [
    "### Import Necessary Files & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmQZUmbKhoi3"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", path)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FtypxpcYhoi9",
    "outputId": "d2e042ab-a9f9-47df-df89-b7dd7fd1107a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Locate the stats CSV file in my computer's memory\n",
    "NBA_PATH = os.path.join(\"nba-salaries-prediction-in-20172018-season\")\n",
    "\n",
    "# # Read the stats CSV file from my computer/Google Drive into a Pandas DataFrame\n",
    "def load_nba_data(nba_path = NBA_PATH):\n",
    "#     csv_path = os.path.join(nba_path, \"2017-18_NBA_salary.csv\")\n",
    "#     return pd.read_csv(csv_path)\n",
    "    return pd.read_csv('/content/drive/My Drive/Colab Notebooks/2017-18_NBA_salary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf69fk3khojB"
   },
   "source": [
    "# Getting & Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiP9guCshojC"
   },
   "source": [
    "## Visualize Dataset Values\n",
    "\n",
    "As is evidenced by the first code block in this section, I decided to preemptively remove the \"Player\" and \"NBA_Country\" features from my dataset.\n",
    "\n",
    "I removed the Player feature because I feared experiencing overfitting in my training. There's a 1:1 ratio between player name and player salary, so my model could easily calculate the salary if it knows the player's name.\n",
    "\n",
    "I removed the NBA_Country feature because regardless of where you're from, you have to live in the US over the course of a season and you get paid in US dollars, so I figured it wouldn't have any effect in either direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "O_LXX6DahojD",
    "outputId": "5869c089-d017-4ff5-d509-367da4fa972a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>NBA_DraftNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>FTr</th>\n",
       "      <th>...</th>\n",
       "      <th>TOV%</th>\n",
       "      <th>USG%</th>\n",
       "      <th>OWS</th>\n",
       "      <th>DWS</th>\n",
       "      <th>WS</th>\n",
       "      <th>WS/48</th>\n",
       "      <th>OBPM</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>BPM</th>\n",
       "      <th>VORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815615</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "      <td>HOU</td>\n",
       "      <td>16</td>\n",
       "      <td>87</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.370</td>\n",
       "      <td>...</td>\n",
       "      <td>18.2</td>\n",
       "      <td>19.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-10.1</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3477600</td>\n",
       "      <td>42</td>\n",
       "      <td>33</td>\n",
       "      <td>GSW</td>\n",
       "      <td>66</td>\n",
       "      <td>937</td>\n",
       "      <td>16.8</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.337</td>\n",
       "      <td>...</td>\n",
       "      <td>19.3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12307692</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>SAC</td>\n",
       "      <td>59</td>\n",
       "      <td>1508</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.140</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3202217</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>CHI</td>\n",
       "      <td>24</td>\n",
       "      <td>656</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.301</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>29.5</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3057240</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>POR</td>\n",
       "      <td>62</td>\n",
       "      <td>979</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.146</td>\n",
       "      <td>...</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Salary  NBA_DraftNumber  Age   Tm   G    MP   PER    TS%   3PAr    FTr  \\\n",
       "0    815615               43   22  HOU  16    87   0.6  0.303  0.593  0.370   \n",
       "1   3477600               42   33  GSW  66   937  16.8  0.608  0.004  0.337   \n",
       "2  12307692               19   36  SAC  59  1508  17.3  0.529  0.193  0.140   \n",
       "3   3202217               13   22  CHI  24   656  14.6  0.499  0.346  0.301   \n",
       "4   3057240               10   20  POR  62   979   8.2  0.487  0.387  0.146   \n",
       "\n",
       "   ...   TOV%  USG%  OWS  DWS   WS  WS/48  OBPM  DBPM   BPM  VORP  \n",
       "0  ...   18.2  19.5 -0.4  0.1 -0.2 -0.121 -10.6   0.5 -10.1  -0.2  \n",
       "1  ...   19.3  17.2  1.7  1.4  3.1  0.160  -0.6   1.3   0.8   0.7  \n",
       "2  ...   12.5  27.6  0.3  1.1  1.4  0.046  -0.6  -1.3  -1.9   0.0  \n",
       "3  ...    9.7  29.5 -0.1  0.5  0.4  0.027  -0.7  -2.0  -2.6  -0.1  \n",
       "4  ...   15.6  15.5 -0.4  1.2  0.8  0.038  -3.7   0.9  -2.9  -0.2  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve NBA stats data from csv downloaded into local directory\n",
    "nba_stats = load_nba_data()\n",
    "\n",
    "# Drop undesired features from original dataset\n",
    "nba_stats = nba_stats.drop([\"Player\", \"NBA_Country\"], axis = 1)\n",
    "nba_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "1_lAgIjhhojJ",
    "outputId": "d97612eb-7b9f-4721-ef2b-95d374c9c761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 485 entries, 0 to 484\n",
      "Data columns (total 26 columns):\n",
      "Salary             485 non-null int64\n",
      "NBA_DraftNumber    485 non-null int64\n",
      "Age                485 non-null int64\n",
      "Tm                 485 non-null object\n",
      "G                  485 non-null int64\n",
      "MP                 485 non-null int64\n",
      "PER                485 non-null float64\n",
      "TS%                483 non-null float64\n",
      "3PAr               483 non-null float64\n",
      "FTr                483 non-null float64\n",
      "ORB%               485 non-null float64\n",
      "DRB%               485 non-null float64\n",
      "TRB%               485 non-null float64\n",
      "AST%               485 non-null float64\n",
      "STL%               485 non-null float64\n",
      "BLK%               485 non-null float64\n",
      "TOV%               483 non-null float64\n",
      "USG%               485 non-null float64\n",
      "OWS                485 non-null float64\n",
      "DWS                485 non-null float64\n",
      "WS                 485 non-null float64\n",
      "WS/48              485 non-null float64\n",
      "OBPM               485 non-null float64\n",
      "DBPM               485 non-null float64\n",
      "BPM                485 non-null float64\n",
      "VORP               485 non-null float64\n",
      "dtypes: float64(20), int64(5), object(1)\n",
      "memory usage: 98.6+ KB\n"
     ]
    }
   ],
   "source": [
    "nba_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "UOapGDtthojN",
    "outputId": "537d6103-77f4-4a0c-8501-0f937676e54e",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>NBA_DraftNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>FTr</th>\n",
       "      <th>ORB%</th>\n",
       "      <th>...</th>\n",
       "      <th>TOV%</th>\n",
       "      <th>USG%</th>\n",
       "      <th>OWS</th>\n",
       "      <th>DWS</th>\n",
       "      <th>WS</th>\n",
       "      <th>WS/48</th>\n",
       "      <th>OBPM</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>BPM</th>\n",
       "      <th>VORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.850000e+02</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.636507e+06</td>\n",
       "      <td>29.451546</td>\n",
       "      <td>26.263918</td>\n",
       "      <td>50.167010</td>\n",
       "      <td>1154.142268</td>\n",
       "      <td>13.260825</td>\n",
       "      <td>0.535387</td>\n",
       "      <td>0.337383</td>\n",
       "      <td>0.263404</td>\n",
       "      <td>4.873814</td>\n",
       "      <td>...</td>\n",
       "      <td>13.140373</td>\n",
       "      <td>18.897320</td>\n",
       "      <td>1.275464</td>\n",
       "      <td>1.176495</td>\n",
       "      <td>2.455258</td>\n",
       "      <td>0.079959</td>\n",
       "      <td>-1.270722</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-1.760206</td>\n",
       "      <td>0.598763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.392602e+06</td>\n",
       "      <td>21.125760</td>\n",
       "      <td>4.272297</td>\n",
       "      <td>24.874872</td>\n",
       "      <td>811.357419</td>\n",
       "      <td>8.769280</td>\n",
       "      <td>0.112352</td>\n",
       "      <td>0.226894</td>\n",
       "      <td>0.294578</td>\n",
       "      <td>4.582810</td>\n",
       "      <td>...</td>\n",
       "      <td>6.115290</td>\n",
       "      <td>5.940536</td>\n",
       "      <td>1.881444</td>\n",
       "      <td>1.034580</td>\n",
       "      <td>2.673670</td>\n",
       "      <td>0.162992</td>\n",
       "      <td>5.026275</td>\n",
       "      <td>2.389343</td>\n",
       "      <td>5.661447</td>\n",
       "      <td>1.245653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.608000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-41.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>-1.063000</td>\n",
       "      <td>-36.500000</td>\n",
       "      <td>-14.300000</td>\n",
       "      <td>-49.200000</td>\n",
       "      <td>-1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.471382e+06</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.202217e+06</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-1.300000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>1819.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.468255e+07</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2898.000000</td>\n",
       "      <td>134.100000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.333000</td>\n",
       "      <td>35.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>66.700000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.713000</td>\n",
       "      <td>68.700000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>54.400000</td>\n",
       "      <td>8.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Salary  NBA_DraftNumber         Age           G           MP  \\\n",
       "count  4.850000e+02       485.000000  485.000000  485.000000   485.000000   \n",
       "mean   6.636507e+06        29.451546   26.263918   50.167010  1154.142268   \n",
       "std    7.392602e+06        21.125760    4.272297   24.874872   811.357419   \n",
       "min    4.608000e+04         1.000000   19.000000    1.000000     1.000000   \n",
       "25%    1.471382e+06        11.000000   23.000000   29.000000   381.000000   \n",
       "50%    3.202217e+06        25.000000   26.000000   59.000000  1134.000000   \n",
       "75%    1.000000e+07        47.000000   29.000000   71.000000  1819.000000   \n",
       "max    3.468255e+07        62.000000   41.000000   79.000000  2898.000000   \n",
       "\n",
       "              PER         TS%        3PAr         FTr        ORB%     ...      \\\n",
       "count  485.000000  483.000000  483.000000  483.000000  485.000000     ...       \n",
       "mean    13.260825    0.535387    0.337383    0.263404    4.873814     ...       \n",
       "std      8.769280    0.112352    0.226894    0.294578    4.582810     ...       \n",
       "min    -41.100000    0.000000    0.000000    0.000000    0.000000     ...       \n",
       "25%      9.800000    0.505500    0.167000    0.155000    1.800000     ...       \n",
       "50%     13.200000    0.545000    0.346000    0.231000    3.200000     ...       \n",
       "75%     16.500000    0.582500    0.481000    0.319500    7.000000     ...       \n",
       "max    134.100000    1.500000    1.000000    5.333000   35.900000     ...       \n",
       "\n",
       "             TOV%        USG%         OWS         DWS          WS       WS/48  \\\n",
       "count  483.000000  485.000000  485.000000  485.000000  485.000000  485.000000   \n",
       "mean    13.140373   18.897320    1.275464    1.176495    2.455258    0.079959   \n",
       "std      6.115290    5.940536    1.881444    1.034580    2.673670    0.162992   \n",
       "min      0.000000    0.000000   -2.300000    0.000000   -1.200000   -1.063000   \n",
       "25%      9.900000   15.000000    0.000000    0.300000    0.300000    0.040000   \n",
       "50%     12.500000   17.900000    0.800000    1.000000    1.800000    0.083000   \n",
       "75%     15.750000   22.200000    2.000000    1.800000    3.600000    0.123000   \n",
       "max     66.700000   45.100000   11.400000    5.600000   15.000000    2.713000   \n",
       "\n",
       "             OBPM        DBPM         BPM        VORP  \n",
       "count  485.000000  485.000000  485.000000  485.000000  \n",
       "mean    -1.270722   -0.489485   -1.760206    0.598763  \n",
       "std      5.026275    2.389343    5.661447    1.245653  \n",
       "min    -36.500000  -14.300000  -49.200000   -1.300000  \n",
       "25%     -2.700000   -1.700000   -3.600000   -0.100000  \n",
       "50%     -1.100000   -0.400000   -1.300000    0.100000  \n",
       "75%      0.400000    1.000000    0.500000    0.900000  \n",
       "max     68.700000    6.800000   54.400000    8.600000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "ybWWQZIwhojR",
    "outputId": "b58c1c95-dc19-47a7-cec6-df87feb04eee",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TOT    55\n",
       "DAL    18\n",
       "MEM    17\n",
       "UTA    17\n",
       "PHI    16\n",
       "ATL    16\n",
       "IND    16\n",
       "GSW    16\n",
       "LAL    15\n",
       "MIL    15\n",
       "ORL    15\n",
       "BOS    15\n",
       "SAS    15\n",
       "MIA    15\n",
       "CHI    14\n",
       "NYK    14\n",
       "DET    14\n",
       "NOP    14\n",
       "POR    14\n",
       "CHO    14\n",
       "PHO    14\n",
       "OKC    14\n",
       "DEN    14\n",
       "WAS    13\n",
       "TOR    13\n",
       "MIN    13\n",
       "HOU    13\n",
       "LAC    12\n",
       "SAC    12\n",
       "BRK    12\n",
       "CLE    10\n",
       "Name: Tm, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_stats['Tm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "colab_type": "code",
    "id": "0LYTt0pohojU",
    "outputId": "ca7fc9a7-4caa-425e-a4cd-a8a6ccd83e7e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIYAAANfCAYAAACsedZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKmj/h84uGUoU8yZPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7766f0eda0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nba_stats.hist(bins=10, figsize = (20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5dTpKc_znZl"
   },
   "source": [
    "## Identify Potentially Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "u7zP06CVhojY",
    "outputId": "c5bd7171-e029-4b39-e486-40328a57db70",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Salary             1.000000\n",
       "WS                 0.592499\n",
       "VORP               0.573840\n",
       "OWS                0.562933\n",
       "MP                 0.506961\n",
       "DWS                0.505500\n",
       "Age                0.336454\n",
       "BPM                0.310478\n",
       "USG%               0.298756\n",
       "G                  0.297700\n",
       "PER                0.269823\n",
       "AST%               0.266459\n",
       "OBPM               0.265614\n",
       "DRB%               0.195899\n",
       "DBPM               0.177818\n",
       "TS%                0.174759\n",
       "WS/48              0.162398\n",
       "TRB%               0.140563\n",
       "BLK%               0.045225\n",
       "STL%               0.035517\n",
       "FTr                0.023494\n",
       "ORB%               0.004221\n",
       "TOV%              -0.043205\n",
       "3PAr              -0.073502\n",
       "NBA_DraftNumber   -0.381686\n",
       "Name: Salary, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = nba_stats.corr()\n",
    "corr_matrix[\"Salary\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tr1MGPgDN_HZ"
   },
   "source": [
    "## Comments\n",
    "\n",
    "This dataset is structured such that each tuple in the dataset is identified by a player name, followed by its corresponding statistcal features. \n",
    "\n",
    "All of the features are numerical with the exception of the \"Tm\" (team) feature. All  31 of its possible values will have to be encoded with a OneHotEncoder when it's time to clean the data.\n",
    "\n",
    "Quite a few of the numerical features have value ranges that extend into the negative territory. Though I'm not exactly sure what effect that may have on my models' performances, I'm intrigued to see the result.\n",
    "\n",
    "Lastly is the correlation matrix in relation to player salary displayed above. Naturally, WS (win shares) and VORP (value over replacement player)--two of the features most directly correlated to a player's contribution to winning games--are leaders in the impact made on salary decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnAfNfNehojc"
   },
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGEZrFVnyTGb"
   },
   "source": [
    "## Check for Missing Feature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "-Dtj7_OZhojd",
    "outputId": "9edef73e-fa2d-4733-f4ef-e25a48c74e48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "      <th>NBA_DraftNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>FTr</th>\n",
       "      <th>...</th>\n",
       "      <th>TOV%</th>\n",
       "      <th>USG%</th>\n",
       "      <th>OWS</th>\n",
       "      <th>DWS</th>\n",
       "      <th>WS</th>\n",
       "      <th>WS/48</th>\n",
       "      <th>OBPM</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>BPM</th>\n",
       "      <th>VORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1579440</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>46080</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>IND</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Salary  NBA_DraftNumber  Age   Tm  G  MP  PER  TS%  3PAr  FTr  ...   \\\n",
       "29  1579440               24   21  DEN  1   2  0.0  NaN   NaN  NaN  ...    \n",
       "37    46080               62   27  IND  1   1  0.0  NaN   NaN  NaN  ...    \n",
       "\n",
       "    TOV%  USG%  OWS  DWS   WS  WS/48  OBPM  DBPM  BPM  VORP  \n",
       "29   NaN   0.0  0.0  0.0  0.0 -0.016  -5.6  -0.9 -6.5   0.0  \n",
       "37   NaN   0.0  0.0  0.0  0.0 -0.001  -5.7  -0.1 -5.9   0.0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate any tuples with missing feature values\n",
    "missing_values = nba_stats[nba_stats.isnull().any(axis = 1)]\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUD5LysNhojh"
   },
   "outputs": [],
   "source": [
    "# Remove the tuples with missing feature values from the dataset\n",
    "nba_stats = nba_stats.dropna(subset = ['TS%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KyqF5oHQuC1"
   },
   "source": [
    "I decided to remove the two player tuples in my dataset that had missing values because they're outliers. One of the players played only 1 minute the whole season and the other only played 2 minutes, so it can automatically be assumed they're making the veteran minimum--if they're still even in the NBA anymore. Their statistics, or lack thereof, wouldn't have contributed anything to the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C23615dEya03"
   },
   "source": [
    "## Prepare Train-Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj9hFFwIhojl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform a random split on the entire dataset; 80% training, 20% testing\n",
    "train_set, test_set = train_test_split(nba_stats, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ao7dzf9Phojn",
    "outputId": "81bbb127-3f6d-4133-98dd-13bb3dabd348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "kmIu9WkLhojs",
    "outputId": "ebe48743-1b5b-49af-d6fe-7be6393916fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWCf3-gohojx"
   },
   "outputs": [],
   "source": [
    "# Remove the output variable (salary) from the training set\n",
    "# and store it in a separate variable\n",
    "nba_stats = train_set.drop('Salary', axis = 1)\n",
    "nba_salary = train_set['Salary'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCoEPSqjyxLc"
   },
   "source": [
    "## Separate Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "fwmzbwI8hoj5",
    "outputId": "9c26582e-c535-4692-d4cf-83bf0eb50055"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBA_DraftNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>G</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>3PAr</th>\n",
       "      <th>FTr</th>\n",
       "      <th>ORB%</th>\n",
       "      <th>DRB%</th>\n",
       "      <th>...</th>\n",
       "      <th>TOV%</th>\n",
       "      <th>USG%</th>\n",
       "      <th>OWS</th>\n",
       "      <th>DWS</th>\n",
       "      <th>WS</th>\n",
       "      <th>WS/48</th>\n",
       "      <th>OBPM</th>\n",
       "      <th>DBPM</th>\n",
       "      <th>BPM</th>\n",
       "      <th>VORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>159</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.513</td>\n",
       "      <td>10.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>1151</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.503</td>\n",
       "      <td>12.2</td>\n",
       "      <td>26.9</td>\n",
       "      <td>...</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>77</td>\n",
       "      <td>2121</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.299</td>\n",
       "      <td>3.1</td>\n",
       "      <td>15.5</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>19.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>62</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>1529</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.4</td>\n",
       "      <td>16.2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>62</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>2180</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.211</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.6</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NBA_DraftNumber  Age   G    MP   PER    TS%   3PAr    FTr  ORB%  DRB%  \\\n",
       "413               48   22  29   159  16.6  0.575  0.000  0.513  10.8  13.0   \n",
       "25                 2   35  46  1151  14.3  0.657  0.000  0.503  12.2  26.9   \n",
       "222               15   22  77  2121  12.2  0.539  0.465  0.299   3.1  15.5   \n",
       "397               62   25  66  1529  12.8  0.561  0.121  0.532   5.4  16.2   \n",
       "233               62   28  69  2180  10.9  0.518  0.574  0.211   1.5  12.6   \n",
       "\n",
       "     ...   TOV%  USG%  OWS  DWS   WS  WS/48  OBPM  DBPM  BPM  VORP  \n",
       "413  ...    5.9  13.7  0.4  0.2  0.6  0.180  -0.9   1.3  0.5   0.1  \n",
       "25   ...   20.5  10.6  1.9  0.8  2.6  0.110  -1.4   1.0 -0.4   0.5  \n",
       "222  ...    8.8  19.4  1.6  2.2  3.7  0.085  -0.6  -0.5 -1.1   0.5  \n",
       "397  ...   13.1  15.0  1.4  1.4  2.8  0.087  -2.2   1.2 -1.0   0.4  \n",
       "233  ...    9.8  18.0  0.4  1.6  1.9  0.043  -0.9  -0.2 -1.1   0.5  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and store all numerical features into a variable\n",
    "nba_stats_num = nba_stats.drop(['Tm'], axis = 1)\n",
    "nba_stats_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "7lteImRJhoj8",
    "outputId": "89ec96a7-dd22-40fb-da2b-329ea5e844ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>OKC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>WAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tm\n",
       "413  OKC\n",
       "25   PHO\n",
       "222  WAS\n",
       "397  CHI\n",
       "233  CHI"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and store all categorical features into a variable\n",
    "nba_stats_cat = nba_stats[['Tm']]\n",
    "nba_stats_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FITxHlR6xtbY"
   },
   "source": [
    "## Future Encoders File\n",
    "I was having trouble importing \"future_encoders.py\" into my notebook for encoding purposes, so I copied and pasted the necessary elements of the file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjIPzNFPo4MQ"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numbers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import six\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES\n",
    "from sklearn.preprocessing.label import LabelEncoder\n",
    "\n",
    "\n",
    "BOUNDS_THRESHOLD = 1e-7\n",
    "\n",
    "\n",
    "zip = six.moves.zip\n",
    "map = six.moves.map\n",
    "range = six.moves.range\n",
    "\n",
    "__all__ = [\n",
    "    'OneHotEncoder',\n",
    "]\n",
    "\n",
    "\n",
    "def _argmax(arr_or_spmatrix, axis=None):\n",
    "    return arr_or_spmatrix.argmax(axis=axis)\n",
    "\n",
    "\n",
    "def _handle_zeros_in_scale(scale, copy=True):\n",
    "    ''' Makes sure that whenever scale is zero, we handle it correctly.\n",
    "\n",
    "    This happens in most scalers when we have constant features.'''\n",
    "\n",
    "    # if we are fitting on 1D arrays, scale might be a scalar\n",
    "    if np.isscalar(scale):\n",
    "        if scale == .0:\n",
    "            scale = 1.\n",
    "        return scale\n",
    "    elif isinstance(scale, np.ndarray):\n",
    "        if copy:\n",
    "            # New array to avoid side-effects\n",
    "            scale = scale.copy()\n",
    "        scale[scale == 0.0] = 1.0\n",
    "        return scale\n",
    "\n",
    "\n",
    "def _transform_selected(X, transform, selected=\"all\", copy=True):\n",
    "    \"\"\"Apply a transform function to portion of selected features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "        Dense array or sparse matrix.\n",
    "\n",
    "    transform : callable\n",
    "        A callable transform(X) -> X_transformed\n",
    "\n",
    "    copy : boolean, optional\n",
    "        Copy X even if it could be avoided.\n",
    "\n",
    "    selected: \"all\" or array of indices or mask\n",
    "        Specify which features to apply the transform to.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array or sparse matrix, shape=(n_samples, n_features_new)\n",
    "    \"\"\"\n",
    "    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n",
    "\n",
    "    if isinstance(selected, six.string_types) and selected == \"all\":\n",
    "        return transform(X)\n",
    "\n",
    "    if len(selected) == 0:\n",
    "        return X\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    ind = np.arange(n_features)\n",
    "    sel = np.zeros(n_features, dtype=bool)\n",
    "    sel[np.asarray(selected)] = True\n",
    "    not_sel = np.logical_not(sel)\n",
    "    n_selected = np.sum(sel)\n",
    "\n",
    "    if n_selected == 0:\n",
    "        # No features selected.\n",
    "        return X\n",
    "    elif n_selected == n_features:\n",
    "        # All features selected.\n",
    "        return transform(X)\n",
    "    else:\n",
    "        X_sel = transform(X[:, ind[sel]])\n",
    "        X_not_sel = X[:, ind[not_sel]]\n",
    "\n",
    "        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n",
    "            return sparse.hstack((X_sel, X_not_sel))\n",
    "        else:\n",
    "            return np.hstack((X_sel, X_not_sel))\n",
    "\n",
    "\n",
    "class _BaseEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base class for encoders that includes the code to categorize and\n",
    "    transform the input features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _fit(self, X, handle_unknown='error'):\n",
    "\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.categories != 'auto':\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "            if len(self.categories) != n_features:\n",
    "                raise ValueError(\"Shape mismatch: if n_values is an array,\"\n",
    "                                 \" it has to be of shape (n_features,).\")\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                if handle_unknown == 'error':\n",
    "                    valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                    if not np.all(valid_mask):\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(self.categories[i])\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "    def _transform(self, X, handle_unknown='error'):\n",
    "\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        _, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            valid_mask = np.in1d(Xi, self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    Xi = Xi.copy()\n",
    "                    Xi[~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n",
    "\n",
    "        return X_int, X_mask\n",
    "\n",
    "\n",
    "WARNING_MSG = (\n",
    "    \"The handling of integer data will change in the future. Currently, the \"\n",
    "    \"categories are determined based on the range [0, max(values)], while \"\n",
    "    \"in the future they will be determined based on the unique values.\\n\"\n",
    "    \"If you want the future behaviour, you can specify \\\"categories='auto'\\\".\"\n",
    ")\n",
    "\n",
    "\n",
    "class OneHotEncoder(_BaseEncoder):\n",
    "    def __init__(self, n_values=None, categorical_features=None,\n",
    "                 categories=None, sparse=True, dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self._categories = categories\n",
    "        if categories is None:\n",
    "            self.categories = 'auto'\n",
    "        else:\n",
    "            self.categories = categories\n",
    "        self.sparse = sparse\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "        if n_values is not None:\n",
    "            pass\n",
    "            # warnings.warn(\"Deprecated\", DeprecationWarning)\n",
    "        else:\n",
    "            n_values = \"auto\"\n",
    "        self._deprecated_n_values = n_values\n",
    "\n",
    "        if categorical_features is not None:\n",
    "            pass\n",
    "            # warnings.warn(\"Deprecated\", DeprecationWarning)\n",
    "        else:\n",
    "            categorical_features = \"all\"\n",
    "        self._deprecated_categorical_features = categorical_features\n",
    "\n",
    "    # Deprecated keywords\n",
    "\n",
    "    @property\n",
    "    def n_values(self):\n",
    "        warnings.warn(\"The 'n_values' parameter is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        return self._deprecated_n_values\n",
    "\n",
    "    @n_values.setter\n",
    "    def n_values(self, value):\n",
    "        warnings.warn(\"The 'n_values' parameter is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        self._deprecated_n_values = value\n",
    "\n",
    "    @property\n",
    "    def categorical_features(self):\n",
    "        warnings.warn(\"The 'categorical_features' parameter is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        return self._deprecated_categorical_features\n",
    "\n",
    "    @categorical_features.setter\n",
    "    def categorical_features(self, value):\n",
    "        warnings.warn(\"The 'categorical_features' parameter is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        self._deprecated_categorical_features = value\n",
    "\n",
    "    # Deprecated attributes\n",
    "\n",
    "    @property\n",
    "    def active_features_(self):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        warnings.warn(\"The 'active_features_' attribute is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        return self._active_features_\n",
    "\n",
    "    @property\n",
    "    def feature_indices_(self):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        warnings.warn(\"The 'feature_indices_' attribute is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        return self._feature_indices_\n",
    "\n",
    "    @property\n",
    "    def n_values_(self):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        warnings.warn(\"The 'n_values_' attribute is deprecated.\",\n",
    "                      DeprecationWarning)\n",
    "        return self._n_values_\n",
    "\n",
    "    def _handle_deprecations(self, X):\n",
    "\n",
    "        user_set_categories = False\n",
    "\n",
    "        if self._categories is not None:\n",
    "            self._legacy_mode = False\n",
    "            user_set_categories = True\n",
    "\n",
    "        elif self._deprecated_n_values != 'auto':\n",
    "            msg = (\n",
    "                \"Passing 'n_values' is deprecated and will be removed in a \"\n",
    "                \"future release. You can use the 'categories' keyword instead.\"\n",
    "                \" 'n_values=n' corresponds to 'n_values=[range(n)]'.\")\n",
    "            warnings.warn(msg, DeprecationWarning)\n",
    "\n",
    "            # we internally translate this to the correct categories\n",
    "            # and don't use legacy mode\n",
    "            X = check_array(X, dtype=np.int)\n",
    "\n",
    "            if isinstance(self._deprecated_n_values, numbers.Integral):\n",
    "                n_features = X.shape[1]\n",
    "                self.categories = [\n",
    "                    list(range(self._deprecated_n_values))\n",
    "                    for _ in range(n_features)]\n",
    "                n_values = np.empty(n_features, dtype=np.int)\n",
    "                n_values.fill(self._deprecated_n_values)\n",
    "            else:\n",
    "                try:\n",
    "                    n_values = np.asarray(self._deprecated_n_values, dtype=int)\n",
    "                    self.categories = [list(range(i))\n",
    "                                       for i in self._deprecated_n_values]\n",
    "                except (ValueError, TypeError):\n",
    "                    raise TypeError(\n",
    "                        \"Wrong type for parameter `n_values`. Expected 'auto',\"\n",
    "                        \" int or array of ints, got %r\".format(type(X)))\n",
    "\n",
    "            self._n_values_ = n_values\n",
    "            n_values = np.hstack([[0], n_values])\n",
    "            indices = np.cumsum(n_values)\n",
    "            self._feature_indices_ = indices\n",
    "\n",
    "            self._legacy_mode = False\n",
    "\n",
    "        else:  # n_values = 'auto'\n",
    "            if self.handle_unknown == 'ignore':\n",
    "                # no change in behaviour, no need to raise deprecation warning\n",
    "                self._legacy_mode = False\n",
    "            else:\n",
    "\n",
    "                # check if we have integer or categorical input\n",
    "                try:\n",
    "                    X = check_array(X, dtype=np.int)\n",
    "                except ValueError:\n",
    "                    self._legacy_mode = False\n",
    "                else:\n",
    "                    warnings.warn(WARNING_MSG, DeprecationWarning)\n",
    "                    self._legacy_mode = True\n",
    "\n",
    "        if (not isinstance(self._deprecated_categorical_features,\n",
    "                           six.string_types)\n",
    "                or (isinstance(self._deprecated_categorical_features,\n",
    "                               six.string_types)\n",
    "                    and self._deprecated_categorical_features != 'all')):\n",
    "            if user_set_categories:\n",
    "                raise ValueError(\n",
    "                    \"The 'categorical_features' keyword is deprecated, and \"\n",
    "                    \"cannot be used together with specifying 'categories'.\")\n",
    "            warnings.warn(\"The 'categorical_features' keyword is deprecated.\",\n",
    "                          DeprecationWarning)\n",
    "            self._legacy_mode = True\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit OneHotEncoder to X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        self._handle_deprecations(X)\n",
    "\n",
    "        if self._legacy_mode:\n",
    "            # TODO not with _transform_selected ??\n",
    "            self._legacy_fit_transform(X)\n",
    "            return self\n",
    "        else:\n",
    "            self._fit(X, handle_unknown=self.handle_unknown)\n",
    "            return self\n",
    "\n",
    "    def _legacy_fit_transform(self, X):\n",
    "        \"\"\"Assumes X contains only categorical features.\"\"\"\n",
    "        self_n_values = self._deprecated_n_values\n",
    "        dtype = getattr(X, 'dtype', None)\n",
    "        X = check_array(X, dtype=np.int)\n",
    "        if np.any(X < 0):\n",
    "            raise ValueError(\"X needs to contain only non-negative integers.\")\n",
    "        n_samples, n_features = X.shape\n",
    "        if (isinstance(self_n_values, six.string_types) and\n",
    "                self_n_values == 'auto'):\n",
    "            n_values = np.max(X, axis=0) + 1\n",
    "        elif isinstance(self_n_values, numbers.Integral):\n",
    "            if (np.max(X, axis=0) >= self_n_values).any():\n",
    "                raise ValueError(\"Feature out of bounds for n_values=%d\"\n",
    "                                 % self_n_values)\n",
    "            n_values = np.empty(n_features, dtype=np.int)\n",
    "            n_values.fill(self_n_values)\n",
    "        else:\n",
    "            try:\n",
    "                n_values = np.asarray(self_n_values, dtype=int)\n",
    "            except (ValueError, TypeError):\n",
    "                raise TypeError(\"Wrong type for parameter `n_values`. Expected\"\n",
    "                                \" 'auto', int or array of ints, got %r\"\n",
    "                                % type(X))\n",
    "            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:\n",
    "                raise ValueError(\"Shape mismatch: if n_values is an array,\"\n",
    "                                 \" it has to be of shape (n_features,).\")\n",
    "\n",
    "        self._n_values_ = n_values\n",
    "        self.categories_ = [np.arange(n_val - 1, dtype=dtype)\n",
    "                            for n_val in n_values]\n",
    "        n_values = np.hstack([[0], n_values])\n",
    "        indices = np.cumsum(n_values)\n",
    "        self._feature_indices_ = indices\n",
    "\n",
    "        column_indices = (X + indices[:-1]).ravel()\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)\n",
    "        data = np.ones(n_samples * n_features)\n",
    "        out = sparse.coo_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "\n",
    "        if (isinstance(self_n_values, six.string_types) and\n",
    "                self_n_values == 'auto'):\n",
    "            mask = np.array(out.sum(axis=0)).ravel() != 0\n",
    "            active_features = np.where(mask)[0]\n",
    "            out = out[:, active_features]\n",
    "            self._active_features_ = active_features\n",
    "\n",
    "            self.categories_ = [\n",
    "                np.unique(X[:, i]).astype(dtype) if dtype else np.unique(X[:, i])\n",
    "                for i in range(n_features)]\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        return out if self.sparse else out.toarray()\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit OneHotEncoder to X, then transform X.\n",
    "\n",
    "        Equivalent to self.fit(X).transform(X), but more convenient and more\n",
    "        efficient. See fit for the parameters, transform for the return value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            Input array of type int.\n",
    "        \"\"\"\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        self._handle_deprecations(X)\n",
    "\n",
    "        if self._legacy_mode:\n",
    "            return _transform_selected(X, self._legacy_fit_transform,\n",
    "                                       self._deprecated_categorical_features,\n",
    "                                       copy=True)\n",
    "        else:\n",
    "            return self.fit(X).transform(X)\n",
    "\n",
    "    def _legacy_transform(self, X):\n",
    "        \"\"\"Assumes X contains only categorical features.\"\"\"\n",
    "        self_n_values = self._deprecated_n_values\n",
    "        X = check_array(X, dtype=np.int)\n",
    "        if np.any(X < 0):\n",
    "            raise ValueError(\"X needs to contain only non-negative integers.\")\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        indices = self._feature_indices_\n",
    "        if n_features != indices.shape[0] - 1:\n",
    "            raise ValueError(\"X has different shape than during fitting.\"\n",
    "                             \" Expected %d, got %d.\"\n",
    "                             % (indices.shape[0] - 1, n_features))\n",
    "\n",
    "        # We use only those categorical features of X that are known using fit.\n",
    "        # i.e lesser than n_values_ using mask.\n",
    "        # This means, if self.handle_unknown is \"ignore\", the row_indices and\n",
    "        # col_indices corresponding to the unknown categorical feature are\n",
    "        # ignored.\n",
    "        mask = (X < self._n_values_).ravel()\n",
    "        if np.any(~mask):\n",
    "            if self.handle_unknown not in ['error', 'ignore']:\n",
    "                raise ValueError(\"handle_unknown should be either error or \"\n",
    "                                 \"unknown got %s\" % self.handle_unknown)\n",
    "            if self.handle_unknown == 'error':\n",
    "                raise ValueError(\"unknown categorical feature present %s \"\n",
    "                                 \"during transform.\" % X.ravel()[~mask])\n",
    "\n",
    "        column_indices = (X + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(np.sum(mask))\n",
    "        out = sparse.coo_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if (isinstance(self_n_values, six.string_types) and\n",
    "                self_n_values == 'auto'):\n",
    "            out = out[:, self._active_features_]\n",
    "\n",
    "        return out if self.sparse else out.toarray()\n",
    "\n",
    "    def _transform_new(self, X):\n",
    "        \"\"\"New implementation assuming categorical input\"\"\"\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        feature_indices = np.cumsum(n_values)\n",
    "\n",
    "        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n",
    "        indptr = X_mask.sum(axis=1).cumsum()\n",
    "        indptr = np.insert(indptr, 0, 0)\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csr_matrix((data, indices, indptr),\n",
    "                                shape=(n_samples, feature_indices[-1]),\n",
    "                                dtype=self.dtype)\n",
    "        if not self.sparse:\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix if sparse=True else a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        if not self._legacy_mode:\n",
    "            return self._transform_new(X)\n",
    "        else:\n",
    "            return _transform_selected(X, self._legacy_transform,\n",
    "                                       self._deprecated_categorical_features,\n",
    "                                       copy=True)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Convert back the data to the original representation.\n",
    "\n",
    "        In case unknown categories are encountered (all zero's in the\n",
    "        one-hot encoding), ``None`` is used to represent this category.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
    "            The transformed data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_tr : array-like, shape [n_samples, n_features]\n",
    "            Inverse transformed array.\n",
    "\n",
    "        \"\"\"\n",
    "        # if self._legacy_mode:\n",
    "        #     raise ValueError(\"only supported for categorical features\")\n",
    "\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        n_features = len(self.categories_)\n",
    "        n_transformed_features = sum([len(cats) for cats in self.categories_])\n",
    "\n",
    "        # validate shape of passed X\n",
    "        msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n",
    "               \"columns, got {1}.\")\n",
    "        if X.shape[1] != n_transformed_features:\n",
    "            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n",
    "\n",
    "        # create resulting array of appropriate dtype\n",
    "        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n",
    "        X_tr = np.empty((n_samples, n_features), dtype=dt)\n",
    "\n",
    "        j = 0\n",
    "        found_unknown = {}\n",
    "\n",
    "        for i in range(n_features):\n",
    "            n_categories = len(self.categories_[i])\n",
    "            sub = X[:, j:j + n_categories]\n",
    "\n",
    "            # for sparse X argmax returns 2D matrix, ensure 1D array\n",
    "            labels = np.asarray(_argmax(sub, axis=1)).flatten()\n",
    "            X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "            if self.handle_unknown == 'ignore':\n",
    "                # ignored unknown categories: we have a row of all zero's\n",
    "                unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n",
    "                if unknown.any():\n",
    "                    found_unknown[i] = unknown\n",
    "\n",
    "            j += n_categories\n",
    "\n",
    "        # if ignored are found: potentially need to upcast result to\n",
    "        # insert None values\n",
    "        if found_unknown:\n",
    "            if X_tr.dtype != object:\n",
    "                X_tr = X_tr.astype(object)\n",
    "\n",
    "            for idx, mask in found_unknown.items():\n",
    "                X_tr[mask, idx] = None\n",
    "\n",
    "        return X_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBwSNZRbzAg4"
   },
   "source": [
    "## Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02VeOMbAhokA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform OneHotEncoding on the Tm (team) categorical feature in the dataset.\n",
    "# This will allow for the conversion of the string values to numbers\n",
    "# for pipelining purposes\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "nba_stats_cat_1hot = cat_encoder.fit_transform(nba_stats_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "GsPQg9GthokD",
    "outputId": "f0bd1922-bc4a-4c95-a5d5-ef5eb83dfd74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20)\t1.0\n",
      "  (1, 23)\t1.0\n",
      "  (2, 30)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 3)\t1.0\n",
      "  (5, 17)\t1.0\n",
      "  (6, 16)\t1.0\n",
      "  (7, 2)\t1.0\n",
      "  (8, 28)\t1.0\n",
      "  (9, 28)\t1.0\n",
      "  (10, 1)\t1.0\n",
      "  (11, 4)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 9)\t1.0\n",
      "  (14, 19)\t1.0\n",
      "  (15, 14)\t1.0\n",
      "  (16, 13)\t1.0\n",
      "  (17, 30)\t1.0\n",
      "  (18, 30)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 26)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 2)\t1.0\n",
      "  (23, 29)\t1.0\n",
      "  (24, 11)\t1.0\n",
      "  :\t:\n",
      "  (361, 21)\t1.0\n",
      "  (362, 21)\t1.0\n",
      "  (363, 0)\t1.0\n",
      "  (364, 13)\t1.0\n",
      "  (365, 0)\t1.0\n",
      "  (366, 7)\t1.0\n",
      "  (367, 14)\t1.0\n",
      "  (368, 4)\t1.0\n",
      "  (369, 7)\t1.0\n",
      "  (370, 23)\t1.0\n",
      "  (371, 29)\t1.0\n",
      "  (372, 28)\t1.0\n",
      "  (373, 10)\t1.0\n",
      "  (374, 5)\t1.0\n",
      "  (375, 9)\t1.0\n",
      "  (376, 30)\t1.0\n",
      "  (377, 17)\t1.0\n",
      "  (378, 25)\t1.0\n",
      "  (379, 10)\t1.0\n",
      "  (380, 25)\t1.0\n",
      "  (381, 18)\t1.0\n",
      "  (382, 2)\t1.0\n",
      "  (383, 28)\t1.0\n",
      "  (384, 28)\t1.0\n",
      "  (385, 20)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(nba_stats_cat_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOSS0V4KyIQN"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icVKy9ZHhokH"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoKs88LehokJ"
   },
   "outputs": [],
   "source": [
    "# Create two separate pipelines for data cleaning, one for\n",
    "# numerical features and the other for categorical features\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_attribs = list(nba_stats_num)\n",
    "cat_attribs = list(nba_stats_cat)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNQeIFjvhokN"
   },
   "outputs": [],
   "source": [
    "# Join the numerical and categorical pipelines together\n",
    "# to form a unified pipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "full_pipeline = FeatureUnion(transformer_list = [\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Y7ObnrKwhokP",
    "outputId": "465ce411-63ca-4db5-e3f2-74c6cf9de4d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 55)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate and store a cleaned dataset for model training and evaluation\n",
    "nba_stats_cleaned = full_pipeline.fit_transform(nba_stats)\n",
    "nba_stats_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muBMO1EwhokS"
   },
   "source": [
    "# Model Selection, Training, & Evaluation (Non-neural Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iGmYlF8AhokU"
   },
   "source": [
    "## Evaluate Performance on Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcTFtiEghokU"
   },
   "outputs": [],
   "source": [
    "# Import all necessary classes & methods to\n",
    "# evaluate model training scores\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIjshgudhokf"
   },
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "bJF1eeEChoki",
    "outputId": "31f77317-4557-47db-9ae4-1f26d154e53e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Cross-validate mean squared error scores\n",
    "forest_scores = cross_val_score(forest_reg, nba_stats_cleaned, nba_salary,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Si60TMn3hokq",
    "outputId": "ff9f696a-0a64-4cce-a8f8-ffef9f9fea79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 5515252.036841055\n",
      "Standard deviation: 918847.3599394716\n"
     ]
    }
   ],
   "source": [
    "# Derive and display RMSE metric from previously cross-validated scores\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6OGf9SnHj9L"
   },
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "yNC3DdI_hokw",
    "outputId": "a1b60feb-b453-4118-e8ba-f245ef01f10f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f7760897d30>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f775f202080>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=150),\n",
    "        'max_features': randint(low=1, high=25),\n",
    "    }\n",
    "\n",
    "# Tune the hyperparameters to improve the random forest regressor\n",
    "# model's performance.\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', \n",
    "                                random_state=42)\n",
    "\n",
    "# Fit the training set to the improved model\n",
    "rnd_search.fit(nba_stats_cleaned, nba_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "2deLVDFphok2",
    "outputId": "6b1229ef-4999-47f7-f7cd-f822e5372888"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 22, 'n_estimators': 53}"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify optimal hyperparameter structure\n",
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUtzdzPg_S9F"
   },
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4sbVYYfhok7"
   },
   "outputs": [],
   "source": [
    "final_model = rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pc5gbXSthok8"
   },
   "outputs": [],
   "source": [
    "X_test = test_set.drop(\"Salary\", axis=1)\n",
    "y_test = test_set[\"Salary\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xu6EoW8shok-"
   },
   "outputs": [],
   "source": [
    "# Clean the data set with the pipeline\n",
    "X_test_cleaned = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6US0DxBMholB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Derive RMSE metric from test set evaluation\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9rOmIJMDholD",
    "outputId": "f0f17534-3a1f-476c-f0bf-3c53e0385cf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4174096.2823522706"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynTVqh0jholI"
   },
   "source": [
    "# Model Selection, Training, & Evaluation (Neural Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plQTppBD2Hnq"
   },
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGteo8kg2H82"
   },
   "outputs": [],
   "source": [
    "X_val = nba_stats_cleaned[:40]\n",
    "partial_X_train = nba_stats_cleaned[40:]\n",
    "\n",
    "y_val = nba_salary[:40]\n",
    "partial_y_train = nba_salary[40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d3DBP3u9Xyh"
   },
   "source": [
    "## Neural Net Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "21QtS2kDholI",
    "outputId": "259c923d-7249-45d5-90f9-ba0e8d2267a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4VwSP8PholL"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Create model-building functions to instantiate multiple models\n",
    "# with minimal code duplication\n",
    "\n",
    "def build_one_layer_model(nodes, dataset): \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(nodes, activation='relu', input_shape=(dataset.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_two_layer_model(nodes, dataset):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(nodes, activation='relu', input_shape=(dataset.shape[1],)))\n",
    "    model.add(layers.Dense(nodes, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_three_layer_model(nodes, dataset):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(nodes, activation='relu', input_shape=(dataset.shape[1],)))\n",
    "    model.add(layers.Dense(nodes, activation='relu'))\n",
    "    model.add(layers.Dense(nodes, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvvET1pS-Z9y"
   },
   "outputs": [],
   "source": [
    "# Number of nodes that can exist in a model's hidden layer\n",
    "node_count = [32, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FnhPI1r9f2f"
   },
   "source": [
    "## Neural Net Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAZn0n1WholN"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "def train_and_validate_model(model):\n",
    "    model.fit(partial_X_train,\n",
    "            partial_y_train,\n",
    "            epochs=8,\n",
    "            batch_size=512,\n",
    "            validation_data=(X_val, y_val))\n",
    "  \n",
    "    results = model.evaluate(X_val, y_val)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8YMArOoholP"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "def test_model(model):\n",
    "    model.fit(nba_stats_cleaned,\n",
    "            nba_salary,\n",
    "            epochs=8,\n",
    "            batch_size=512)\n",
    "    \n",
    "    X_test_cleaned = full_pipeline.transform(X_test)\n",
    "  \n",
    "    results = model.evaluate(X_test_cleaned, y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuKOi-xz-iJQ"
   },
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "colab_type": "code",
    "id": "9GMVezOEholT",
    "outputId": "16691e17-f38c-464a-e538-76b097169bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1 , 32 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 301us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 20us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 7us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "40/40 [==============================] - 0s 128us/step\n",
      "RMSE: 11157853.75 \n",
      "\n",
      "\n",
      "\n",
      "Iteration # 2 , 128 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 308us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542517.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 19us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 12us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757536051200.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 12us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542515.0000\n",
      "40/40 [==============================] - 0s 84us/step\n",
      "RMSE: 11157853.45 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "# when 1 hidden layer is present\n",
    "for i, num in enumerate(node_count):\n",
    "  print(\"Iteration #\", (i+1), \",\", num, \"nodes\")\n",
    "  \n",
    "  model = build_one_layer_model(num, partial_X_train)\n",
    "  results = train_and_validate_model(model)\n",
    "  \n",
    "  rmse = np.sqrt(results[0])\n",
    "  print(\"RMSE:\", round(rmse, 2), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zarsv0Wn-pvd"
   },
   "source": [
    "### Two Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "colab_type": "code",
    "id": "nyq2CGY0holW",
    "outputId": "f2e2215c-bbb7-47e4-8435-a8190d797a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1 , 32 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 416us/step - loss: 99757561217024.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757561217024.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 10us/step - loss: 99757561217024.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542515.0000\n",
      "40/40 [==============================] - 0s 145us/step\n",
      "RMSE: 11157853.45 \n",
      "\n",
      "\n",
      "\n",
      "Iteration # 2 , 128 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 509us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 20us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497680138240.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 17us/step - loss: 99757536051200.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497680138240.0000 - val_mean_absolute_error: 7542514.5000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 13us/step - loss: 99757527662592.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497663361024.0000 - val_mean_absolute_error: 7542513.5000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 22us/step - loss: 99757519273984.0000 - mean_absolute_error: 6666722.5000 - val_loss: 124497654972416.0000 - val_mean_absolute_error: 7542513.5000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 18us/step - loss: 99757510885376.0000 - mean_absolute_error: 6666721.5000 - val_loss: 124497654972416.0000 - val_mean_absolute_error: 7542513.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 16us/step - loss: 99757502496768.0000 - mean_absolute_error: 6666721.5000 - val_loss: 124497654972416.0000 - val_mean_absolute_error: 7542513.0000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 23us/step - loss: 99757485719552.0000 - mean_absolute_error: 6666721.5000 - val_loss: 124497629806592.0000 - val_mean_absolute_error: 7542512.0000\n",
      "40/40 [==============================] - 0s 79us/step\n",
      "RMSE: 11157850.74 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "# when 2 hidden layers are present\n",
    "for i, num in enumerate(node_count):\n",
    "  print(\"Iteration #\", (i+1), \",\", num, \"nodes\")\n",
    "  \n",
    "  model = build_two_layer_model(num, partial_X_train)\n",
    "  results = train_and_validate_model(model)\n",
    "  \n",
    "  rmse = np.sqrt(results[0])\n",
    "  print(\"RMSE:\", round(rmse, 2), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiBiTmvA-wHG"
   },
   "source": [
    "### Three Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "colab_type": "code",
    "id": "PlshltdKholZ",
    "outputId": "7cea1792-98a6-42b5-f9a3-7f8f9fb56d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1 , 32 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 658us/step - loss: 99757561217024.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 22us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 8us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497705304064.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 9us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 10us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542516.0000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 18us/step - loss: 99757552828416.0000 - mean_absolute_error: 6666724.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542516.0000\n",
      "40/40 [==============================] - 0s 68us/step\n",
      "RMSE: 11157853.45 \n",
      "\n",
      "\n",
      "\n",
      "Iteration # 2 , 128 nodes\n",
      "Train on 346 samples, validate on 40 samples\n",
      "Epoch 1/8\n",
      "346/346 [==============================] - 0s 752us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666724.5000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 2/8\n",
      "346/346 [==============================] - 0s 29us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497696915456.0000 - val_mean_absolute_error: 7542515.0000\n",
      "Epoch 3/8\n",
      "346/346 [==============================] - 0s 27us/step - loss: 99757544439808.0000 - mean_absolute_error: 6666723.0000 - val_loss: 124497680138240.0000 - val_mean_absolute_error: 7542514.5000\n",
      "Epoch 4/8\n",
      "346/346 [==============================] - 0s 18us/step - loss: 99757527662592.0000 - mean_absolute_error: 6666722.5000 - val_loss: 124497654972416.0000 - val_mean_absolute_error: 7542513.5000\n",
      "Epoch 5/8\n",
      "346/346 [==============================] - 0s 25us/step - loss: 99757519273984.0000 - mean_absolute_error: 6666722.5000 - val_loss: 124497638195200.0000 - val_mean_absolute_error: 7542512.0000\n",
      "Epoch 6/8\n",
      "346/346 [==============================] - 0s 22us/step - loss: 99757502496768.0000 - mean_absolute_error: 6666721.0000 - val_loss: 124497613029376.0000 - val_mean_absolute_error: 7542511.0000\n",
      "Epoch 7/8\n",
      "346/346 [==============================] - 0s 17us/step - loss: 99757468942336.0000 - mean_absolute_error: 6666719.5000 - val_loss: 124497587863552.0000 - val_mean_absolute_error: 7542509.5000\n",
      "Epoch 8/8\n",
      "346/346 [==============================] - 0s 28us/step - loss: 99757452165120.0000 - mean_absolute_error: 6666718.5000 - val_loss: 124497562697728.0000 - val_mean_absolute_error: 7542507.0000\n",
      "40/40 [==============================] - 0s 138us/step\n",
      "RMSE: 11157847.28 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the validation set\n",
    "# when 3 hidden layers are present\n",
    "for i, num in enumerate(node_count):\n",
    "  print(\"Iteration #\", (i+1), \",\", num, \"nodes\")\n",
    "  \n",
    "  model = build_three_layer_model(num, partial_X_train)\n",
    "  results = train_and_validate_model(model)\n",
    "  \n",
    "  rmse = np.sqrt(results[0])\n",
    "  print(\"RMSE:\", round(rmse, 2), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZJ8Dith_B2j"
   },
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "bF9RyMxCholc",
    "outputId": "b24a6e23-58ba-439c-e257-a8a63821edcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "386/386 [==============================] - 0s 749us/step - loss: 102321287593984.0000 - mean_absolute_error: 6757479.5000\n",
      "Epoch 2/8\n",
      "386/386 [==============================] - 0s 29us/step - loss: 102321287593984.0000 - mean_absolute_error: 6757479.0000\n",
      "Epoch 3/8\n",
      "386/386 [==============================] - 0s 24us/step - loss: 102321287593984.0000 - mean_absolute_error: 6757478.0000\n",
      "Epoch 4/8\n",
      "386/386 [==============================] - 0s 22us/step - loss: 102321279205376.0000 - mean_absolute_error: 6757477.5000\n",
      "Epoch 5/8\n",
      "386/386 [==============================] - 0s 20us/step - loss: 102321245650944.0000 - mean_absolute_error: 6757477.0000\n",
      "Epoch 6/8\n",
      "386/386 [==============================] - 0s 19us/step - loss: 102321228873728.0000 - mean_absolute_error: 6757475.5000\n",
      "Epoch 7/8\n",
      "386/386 [==============================] - 0s 21us/step - loss: 102321195319296.0000 - mean_absolute_error: 6757474.0000\n",
      "Epoch 8/8\n",
      "386/386 [==============================] - 0s 19us/step - loss: 102321161764864.0000 - mean_absolute_error: 6757472.0000\n",
      "97/97 [==============================] - 0s 906us/step\n",
      "RMSE: 9257654.74 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model with the\n",
    "# best hyperparamter structure on the test set\n",
    "model = build_three_layer_model(128, nba_stats_cleaned)\n",
    "results = test_model(model)\n",
    "  \n",
    "rmse = np.sqrt(results[0])\n",
    "print(\"RMSE:\", round(rmse, 2), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yE1YyvgPJHG"
   },
   "source": [
    "# Random Forest & Neural Net Performance Observations\n",
    "\n",
    "I was amazed at the results achieved by my baseline and neural network models, and personally I think that shock is for the worse. The clear winner in my case was the random forest regressor. In fact, it performed twice as well as the neural net I used to evaluate test set performance.\n",
    "\n",
    "I believe my surprising results were the fruits of my differing hyperparameter tuning methods. I chose the systematic approach for my random forest, whereas for my neural net I manually performed the tuning using a variety of different layer sizes and quantities.\n",
    "\n",
    "The data from my neural net validation evaluation epochs doesn't suggest that there were any signs of overfitting, and that would've been the only other logical assumption I could conclude from my results. Perhaps this was just a poor neural net construction on my part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cN4H72Lqholf"
   },
   "source": [
    "# Future Work\n",
    "\n",
    "The first thing I'd immediately do given more time would be to perform systematic hyperparameter tuning on my neural net and scrap the manual tuning. I'm starting to be convinced more and more of that being the reason for the disparity in my test set evaluations.\n",
    "\n",
    "Secondly I would experiment with transformed features using PCA on my random forest model, even though it's more geared towards unsupervised learning. It'd be hard to create new features from the ones given in the dataset because of how complex they already are. But PCA would allow me to consolidate these already-complex features and help me derive even more meaning from their structure on a graphical level...and who doesn't love graphs?!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Y82AdBcWz1bZ",
    "eiP9guCshojC",
    "i5dTpKc_znZl",
    "BGEZrFVnyTGb",
    "C23615dEya03",
    "uCoEPSqjyxLc",
    "FITxHlR6xtbY",
    "SBwSNZRbzAg4",
    "DOSS0V4KyIQN",
    "iGmYlF8AhokU",
    "aIjshgudhokf",
    "B6OGf9SnHj9L",
    "cUtzdzPg_S9F",
    "plQTppBD2Hnq",
    "-d3DBP3u9Xyh",
    "7FnhPI1r9f2f",
    "cuKOi-xz-iJQ",
    "Zarsv0Wn-pvd",
    "ZiBiTmvA-wHG",
    "1ZJ8Dith_B2j"
   ],
   "name": "NBA Salary Advanced Stats Predictions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
